# -*- coding: utf-8 -*-
"""VGG16-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JkVyuogKj1RfvGS6wdG2dUjpaP985U2N

###Importing Tools
We start by importing several libraries, which are like tools in a toolbox:

1. numpy: Helps with math.
2. pandas: Helps with handling data.
3. os: Helps with accessing files on the computer.
4. matplotlib and seaborn: Help with drawing graphs.
5. tensorflow and keras: Help with building and training our model.
6. cv2: Used for image processing.
7. regularizers: Helps prevent overfitting in the model.
Optimizers (Adam, RMSprop, SGD, Adamax): Help adjust the learning rate and improve the model during training.
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import keras
from keras.preprocessing import image
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
from keras import regularizers
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax
import joblib  # **import joblib for model saving and loading**

from google.colab import drive
drive.mount('/content/drive')

"""###Loading and Exploring Data
####Loading Data:
1. We load images from a directory that contains pictures of rice leaves.
2. The images are loaded into a dataset, and we print out the class names (types of diseases) to see what we're working with.
"""

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    r"/content/drive/MyDrive/Rice Leaf Disease Images",
    shuffle=True,
    image_size=(150,150),
    batch_size=32
)
class_names = dataset.class_names
print(class_names)
len(dataset)

"""###Exploring Data:
1. We check the shape (size) of the images and labels.
2. We display a few images with their labels to get a sense of the data.
"""

for image_batch, labels_batch in dataset.take(1):
    print(image_batch.shape)
    print(labels_batch.numpy())

for image_batch, label_batch in dataset.take(1):
    for i in range(12):
        ax = plt.subplot(3, 4, i + 1)
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[label_batch[i]])
        plt.axis("off")

"""###Preparing the Data
We prepare our data for training:

1. ImageDataGenerator: This tool helps create more training images by making small changes to the original images (like rotating, shifting, etc.). It also splits the data into training and validation sets.
"""

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.1,  # reserve 10% of the data for validation
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

"""###Generate Training and Validation Data:

We create two sets of data: one for training the model and one for validating (checking) the model.
"""

base_dir= r"/content/drive/MyDrive/Rice Leaf Disease Images"
train_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    color_mode="rgb",
    batch_size=32,
    subset='training',  # set as training data
    class_mode='categorical'
)

validation_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    color_mode="rgb",
    batch_size=32,
    subset='validation',  # set as validation data
    class_mode='categorical'
)

"""###Building the Model
We build our model using a pre-trained model called VGG16:

1. VGG16: This model has already been trained on a large set of images and can help our model learn better.
2. Sequential Model: We add layers to this model step by step.
"""

from tensorflow.keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
               include_top=False,
               input_shape=(150,150,3))

model = Sequential()
model.add(conv_base)
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(5, activation='softmax'))

"""Freezing Layers: We freeze the layers of VGG16 so their weights don't change during training."""

conv_base.trainable = False

"""###Compiling the Model
We compile the model to specify how it will learn and what it will try to optimize:

1. Optimizer: We use Adam to adjust the learning rate.
2. Loss Function: We use categorical cross-entropy to measure how well the model is doing.
3. Metrics: We track accuracy to see how often the model is correct.
"""

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

"""###Training the Model
We train the model using the training data and validate it using the validation data:

Training: We train the model for 10 epochs (rounds of training).
"""

history = model.fit(train_generator, validation_data=validation_generator, epochs=10)

"""###Evaluating the Model
We plot the training and validation accuracy and loss to see how well the model is learning:
"""

fig, ax = plt.subplots(1, 2)

# Retrieve training accuracy, validation accuracy, training loss, and validation loss
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Set the figure size
fig.set_size_inches(12, 4)

# Plot training and validation accuracy with markers
ax[0].plot(train_acc, marker='o')  # Add markers for training accuracy
ax[0].plot(val_acc, marker='o')  # Add markers for validation accuracy
ax[0].set_title('Training Accuracy vs Validation Accuracy')
ax[0].set_ylabel('Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].legend(['Train', 'Validation'], loc='lower right')

# Adjust the y-axis limits for accuracy (closer to 0-1 range)
ax[0].set_ylim([0, 1])  # Accuracy is between 0 and 1
ax[0].set_yticks(np.arange(0, 1.1, 0.1))  # Set tick marks at intervals of 0.1

# Plot training and validation loss with markers
ax[1].plot(train_loss, marker='o')  # Add markers for training loss
ax[1].plot(val_loss, marker='o')  # Add markers for validation loss
ax[1].set_title('Training Loss vs Validation Loss')
ax[1].set_ylabel('Loss')
ax[1].set_xlabel('Epoch')
ax[1].legend(['Train', 'Validation'], loc='upper right')

# Adjust the y-axis limits for loss to minimize unnecessary margins
min_loss = min(min(train_loss), min(val_loss))  # Get the minimum loss value
max_loss = max(max(train_loss), max(val_loss))  # Get the maximum loss value

# Set the y-axis limits close to the minimum and maximum loss values, with a small margin
ax[1].set_ylim([min_loss - 0.1, max_loss + 0.1])

# Set tick marks on the y-axis to show more numbers
ax[1].set_yticks(np.arange(min_loss - 0.1, max_loss + 0.2, 0.1))  # Set tick marks at intervals of 0.1

# Show the plots with tighter layout
plt.tight_layout()
plt.show()

# Evaluate and print accuracy on validation data
print("Accuracy of our model on validation data: ", model.evaluate(validation_generator)[1] * 100, "%")

import matplotlib.pyplot as plt
import numpy as np

# Get class labels
class_labels = list(train_generator.class_indices.keys())

# Get the counts of each class in the training and validation sets
train_counts = np.zeros(len(class_labels))
val_counts = np.zeros(len(class_labels))

for i in range(len(train_generator.classes)):
    train_counts[train_generator.classes[i]] += 1

for i in range(len(validation_generator.classes)):
    val_counts[validation_generator.classes[i]] += 1

# Create the bar chart
x = np.arange(len(class_labels))  # label locations

width = 0.35  # width of the bars

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

train_bars = ax.bar(x - width/2, train_counts, width, label='Training Set', color='blue')
val_bars = ax.bar(x + width/2, val_counts, width, label='Validation Set', color='orange')

# Adding labels and title
ax.set_xlabel('Class Labels')
ax.set_ylabel('Number of Images')
ax.set_title('Number of Images per Class in Training and Validation Sets')
ax.set_xticks(x)
ax.set_xticklabels(class_labels)
ax.legend()

# Show the plot
plt.show()

def extract_data(generator):
    data_list = []
    labels_list = []
    for _ in range(generator.__len__()):
        data, labels = generator.__next__()
        data_list.append(data)
        labels_list.append(labels)
    x = np.vstack(data_list)
    y = np.vstack(labels_list)
    return x, y

x_train, y_train = extract_data(train_generator)
x_test, y_test = extract_data(validation_generator)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support

# Evaluate model on test data
loss = model.evaluate(x_test, y_test)
print("Test Accuracy: {:.2f}%".format(loss[1] * 100))

# Predict the classes for the test set
preds = model.predict(x_test)
y_pred = np.argmax(preds, axis=1)

# Dictionary of class labels
label_dict = {0: 'Bacterialblight', 1: 'Blast', 2: 'Brownspot', 3: 'Healthy', 4: 'Tungro'}

# Visualizing some predictions
figure = plt.figure(figsize=(28, 12))
for i, index in enumerate(np.random.choice(x_test.shape[0], size=24)):
    ax = figure.add_subplot(4, 6, i + 1, xticks=[], yticks=[])
    ax.imshow(np.squeeze(x_test[index]))
    predict_index = label_dict[(y_pred[index])]
    true_index = label_dict[np.argmax(y_test, axis=1)[index]]

    ax.set_title("{} ({})".format(predict_index, true_index),
                 color=("green" if predict_index == true_index else "red"))

# Convert y_test to single-digit labels (actual classes)
y_true = np.argmax(y_test, axis=1)

# Generate classification report for individual precision, recall, F1-score, and support
report = classification_report(y_true, y_pred, target_names=label_dict.values(), output_dict=True)
print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=label_dict.values()))

# Display overall metrics (averages)
overall_metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print("\nOverall Metrics:")
print("Precision: {:.2f}".format(overall_metrics[0]))
print("Recall: {:.2f}".format(overall_metrics[1]))
print("F1-score: {:.2f}".format(overall_metrics[2]))

# Confusion Matrix
CM = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(CM, annot=True, fmt='g', cmap='Purples', cbar=False, xticklabels=label_dict.values(), yticklabels=label_dict.values())
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Print the confusion matrix values
print("Confusion Matrix:\n", CM)

# Bar chart for individual class metrics (Precision, Recall, F1-score, and Support)
labels = list(label_dict.values())
precision = [report[label]['precision'] for label in labels]
recall = [report[label]['recall'] for label in labels]
f1_score = [report[label]['f1-score'] for label in labels]
support = [report[label]['support'] for label in labels]

# Plotting the bar chart
x = np.arange(len(labels))  # Label locations
width = 0.2  # Width of the bars

fig, ax = plt.subplots(figsize=(12, 8))

# Plot bars for each metric
bar1 = ax.bar(x - width, precision, width, label='Precision')
bar2 = ax.bar(x, recall, width, label='Recall')
bar3 = ax.bar(x + width, f1_score, width, label='F1-score')

# Adding labels and title
ax.set_xlabel('Class Labels')
ax.set_ylabel('Metrics')
ax.set_title('Precision, Recall, and F1-score for Each Class')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

# Display the plot
plt.show()